from snakemake.utils import min_version
min_version("5.17.0")

configfile: "config.yaml"
#container: "envs/hand_sandbox.simg"  # only effective when --use-singularity
# removed gloabal singularity so that can use --use-singularity and --use-conda at the same time

#localrules: targets, create_dag

import pandas as pd
import sys
import re
import os
import math
import gzip
import shutil

def gunzip(fname):
    outname = re.sub(".gz$", "", fname)
    if outname == fname:
        sys.exit("config error:", fname, "does not end with .gz")
    # todo: skip if uncompressed file exists (but how to check for previous uncompress integrity
    with gzip.open(fname, 'rb') as f_in:
        with open(outname, 'wb') as f_out:
            shutil.copyfileobj(f_in, f_out)


# umcompress files
if config["GENOME"].endswith(".gz"):
    gunzip(config["GENOME"])
    config["GENOME"] = re.sub(".gz$", "", config["GENOME"])

if config["GTF"].endswith(".gz"):
    gunzip(config["GTF"])
    config["GTF"] = re.sub(".gz$", "", config["GTF"])

if config["VCF"].endswith(".gz"):
    gunzip(config["VCF"])
    config["VCF"] = re.sub(".gz$", "", config["VCF"])

SAMPLES=config['SAMPLES']
GENOME=config["GENOME"]
INDEX=config["INDEX"]
GTF=config["GTF"]
ANNO_TAB=config["ANNO_TAB"]
STRAND=config["STRAND"]
MODE=config["MODE"]
MAX_FDR=config["MAX_FDR"]
MIN_LFC=config["MIN_LFC"]

# priority 0-100, default 0

# get CONTRASTS from meta/contrast.xlsx


# Get contrast name for DESeq2
def get_contrast_fnames (fname):
    df=pd.read_excel(fname)
    CONTRASTS = []
    for j in range(df.shape[1]):
        c1 = df.iloc[0,j]
        c2 = df.iloc[1,j]
        c1 = c1.replace(" ", "")
        c2 = c2.replace(" ", "")
        c1 = re.sub(";$", "", c1)  # remove extra ;
        c2 = re.sub(";$", "", c2)
        c1 = re.sub(";", ".", c1)  # remove extra ;
        c2 = re.sub(";", ".", c2)
        CONTRASTS.append(c1 + "_vs_" + c2)
    # CONTRATS = ['KO_D8_vs_KO_D0', 'WT_D8.KO_D8_vs_WT_D0.KO_D0', 'KO_D0.KO_D2.KO_D8_vs_WT_D0.WT_D2.WT_D8']
    return (CONTRASTS)


def get_contrast_groups (fname):
    df2=pd.read_excel(fname)
    C1S = []; C2S = []
    for j in range(df2.shape[1]):
        c1 = df2.iloc[0, j].strip()
        c2 = df2.iloc[1, j].strip()
        c1 = c1.replace(" ", "")
        c2 = c2.replace(" ", "")
        c1 = re.sub(";$", "", c1)  # remove extra ;
        c2 = re.sub(";$", "", c2)
        c1s = c1.split(";")
        c2s = c2.split(";")
        C1S.append(c1s)
        C2S.append(c2s)
    return ([C1S, C2S])


def get_dict_from_meta (fname):
    df = pd.read_excel(fname)
    d = {}
    for i in range(df.shape[0]):
        sample = df.iloc[i, 0]
        group = df.iloc[i, 1]
        if group in d:
            d[group].append(sample)
        else:
            d[group] = []
            d[group].append(sample)
    return (d)


def swap_list_element(l, d):
    return [swap_list_element(x, d) if isinstance(x, list) else d.get(x, x) for x in l]
    
def flatten(l):
    flat_list = [item for sublist in l for item in sublist]
    return (flat_list)

def collapseL2(L3):
    L2 = []
    for l in L3:
        L2.append(flatten(l))
    return (L2)

def s2b(S):
    K = flatten(S)
    V = ["sorted_reads/"+x+".bam" for x in K]
    return (dict(zip(K, V)))

def G2B_workflow(G, g2s):
    S = swap_list_element(G, g2s)
    S = collapseL2(S)
    s2b_dict = s2b(S)
    BS = swap_list_element(S, s2b_dict)
    B = [",".join(x) for x in BS]
    return (BS, B)



# for DESeq2
if config['START'] in ["FASTQ", "BAM", "COUNT"]:
    CONTRASTS_DE = get_contrast_fnames(config['CONTRAST_DE'])
    #print("CONTRASTS_DE:\n", CONTRASTS_DE)
else:
    CONTRASTS_DE = ["placeholder"]

# for DEXSeq
CONTRASTS_DE_for_DEX = [l.replace('.','_') for l in CONTRASTS_DE]


# for rMATS
if config['START'] in ["FASTQ", "BAM"]:
    G = get_contrast_groups(config['CONTRAST_AS'])
    #print("\nGRPUPS:\n", G[0], "\nVS\n", G[1])

    g2s = get_dict_from_meta(config['META'])
    #print("\nGroup To Sample Mapping:\n", g2s)

    B1S, B1 = G2B_workflow(G[0], g2s)
    B2S, B2 = G2B_workflow(G[1], g2s)

    # print("\nrMATS -b1:\n", B1)
    # print("B1S:", B1S)
    # print("rMATS -b2:\n", B2)
    # print("B2S:", B2S)

    # Get contrast for rMATS
    df2 = pd.read_excel(config['CONTRAST_AS'])
    ASCN = df2.shape[1] # alternative splicing contrast count
    #print("ASCN", ASCN)
else:
    B1 = "placeholder.bam"
    B1S = ["placeholder"]
    B2 = B1
    B2S = B1S
    ASCN = 0



# load modules (have to use """, to keep in one block)
# - alias does not work, have to use $samstat
shell.prefix("""
            # module load star/2.5.3a
            # module load singularity/singularity-current
            # conda activate osr  # have to create conda env osr first
            """)

# Requirements
# inputs in ./fastq/xxx.{fastq,fq}.gz
# named as {sample}.{R1,R2}.{fastq,fq}.gz

# SnakeMake Coding Notes:
# input don't have to be used, just for draw nice DAG
# 07/10/2019 randomized primary alignment

ruleorder: create_dag > DESeq2 > GSEA  > rMATS > FastQC > bam_qc > bamCoverage > feature_count  > samtools_sort > samtools_index > reset

rule targets:
    input:
        # 1. everything listed here will be produced by the pipeline
        # 2. feed {sample}
        fastqc=("fastqc/multiqc_report.html" 
            if config["START"] == "FASTQ" 
            else "Workflow_DAG.all.svg"),

        bam_qc=(expand("bam_qc/idxstats/{sample}.idxstats.txt", sample=SAMPLES) 
            if config["START"] in ["FASTQ", "BAM"]  
            else "Workflow_DAG.all.svg"),

        QoRTs=(expand("bam_qc/QoRTs/{sample}/QC.QORTS_COMPLETED_OK", sample=SAMPLES)
            if config["START"] in ["FASTQ", "BAM"]  
            else "Workflow_DAG.all.svg"),

        QoRTs_MultiPlot=("bam_qc/QoRTs_MultiPlot/plot-basic.pdf"
            if config["START"] in ["FASTQ", "BAM"]  
            else "Workflow_DAG.all.svg"),

        bamCoverage=(expand("bigWig/{sample}.{mode}.cpm.bw", sample=SAMPLES, mode=MODE) 
            if config["START"] in ["FASTQ", "BAM"]  
            else "Workflow_DAG.all.svg"),

        feature_count_strict=(expand("feature_count/counts.s{strand}.strict.txt", strand=STRAND)
            if config["START"] in ["FASTQ", "BAM"] and config["MODE"] == "strict"
            else "Workflow_DAG.all.svg"),

        feature_count_liberal=(expand("feature_count/counts.s{strand}.liberal.txt", strand=STRAND)       
            if config["START"] in ["FASTQ", "BAM"] and config["MODE"] == "liberal"
            else "Workflow_DAG.all.svg"),

        DESeq2="DESeq2/DESeq2.html"
            if config["START"] in ["FASTQ", "BAM", "COUNT"]
            else "Workflow_DAG.all.svg",

        GSEA = expand("log/gsea/{contrast}/{db}.finished", contrast=CONTRASTS_DE, db=config["GSEA_DBS"])
            if config["START"] in ["FASTQ", "BAM", "COUNT"]
            else expand("gsea/{contrast}/{db}.finished", contrast=config["RNKS"], db=config["GSEA_DBS"]), # contrast just a name

        GSEA_compression =  expand("gsea_compressed/{contrast}/{db}.GseaPreranked.tar.gz", contrast=CONTRASTS_DE, db=config["GSEA_DBS"])
            if config["START"] in ["FASTQ", "BAM", "COUNT"]
            else expand("gsea_compressed/{contrast}/{db}.GseaPreranked.tar.gz", contrast=config["RNKS"], db=config["GSEA_DBS"]), # contrast just a name

        rMATS=expand("rMATS.{ascn}/output/Results_JunctionCountsBased/RI.MATS.JC.txt", ascn=range(ASCN))
            if config['AS_ANALYSIS'] and config["START"] in ["FASTQ", "BAM"] 
            else "Workflow_DAG.all.svg",

        DEXSeq_count=expand("DEXSeq_count/{sample}_count.txt", sample=SAMPLES)
            if config['AS_ANALYSIS'] and config["START"] in ["FASTQ", "BAM"]
            else "Workflow_DAG.all.svg",

        DEXSeq=expand("DEXSeq/DEXSeq_{contrast}.xlsx", contrast=CONTRASTS_DE_for_DEX)
            if config['AS_ANALYSIS'] and config["START"] in ["FASTQ", "BAM"]
            else "Workflow_DAG.all.svg",

        SalmonTE=("SalmonTE_output/EXPR.csv"
            if config['TE_ANALYSIS'] and config["START"] == "FASTQ"
            else "Workflow_DAG.all.svg"),

        GATK_ASE=(expand("GATK_ASEReadCounter/{sample}.table", sample=SAMPLES)
            if config["ASE_ANALYSIS"] and config["START"] in ["FASTQ", "BAM"]
            else "Workflow_DAG.all.svg"),

        dag="Workflow_DAG.all.svg", # create DAG
#        organize_results="log/oranize_results.finished",
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 1000   

## fastqc
if config["PAIR_END"]:
    rule FastQC:
        # don't need input, if you agree on not checking them
        # without output, output will not be created
        input:
            r1="fastq/{sample}.R1.fastq.gz",
            r2="fastq/{sample}.R2.fastq.gz",
        output:
            r1="fastqc/details/{sample}.R1_fastqc.html", 
            r2="fastqc/details/{sample}.R2_fastqc.html" 
        priority:
            10
        resources:
            mem_mb=lambda wildcards, attempt: attempt * 1000   
        threads:
            1
        log:
            "log/fastqc/{sample}.log"
        benchmark:
            "log/fastqc/{sample}.tsv"
        container:
            "envs/hand_sandbox.simg"
        envmodules:
            "fastqc/0.11.5"
        shell:
            "mkdir -p fastqc && mkdir -p fastqc/details; "
            "which fastqc &> {log};"
            "fastqc -t {threads} {input} -o fastqc/details &>> {log}"
else:
    rule FastQC:
        # don't need input, if you agree on not checking them
        # without output, output will not be created
        input:
            r1="fastq/{sample}.fastq.gz" 
        output:
            "fastqc/details/{sample}_fastqc.html"
        priority:
            10
        resources:
            mem_mb=lambda wildcards, attempt: attempt * 1000   
        threads:
            1
        log:
            "log/fastqc/{sample}.log"
        benchmark:
            "log/fastqc/{sample}.tsv"
        container:
            "envs/hand_sandbox.simg"
        envmodules:
            "fastqc/0.11.5"
        shell:
            "mkdir -p fastqc && mkdir -p fastqc/details; "
            "which fastqc &> {log};"
            "fastqc -t {threads} {input} -o fastqc/details &>> {log}"


# rule MultiQC
if config["PAIR_END"]:
    rule MultiQC:
        input:
            r1=expand("fastqc/details/{sample}.R1_fastqc.html", sample=SAMPLES),
            r2=expand("fastqc/details/{sample}.R2_fastqc.html", sample=SAMPLES) 
        output:
            "fastqc/multiqc_report.html"
        priority:
            10
        resources:
            mem_mb=lambda wildcards, attempt: attempt * 1000   
        threads:
            1
        log:
            "log/multiqc/multiqc.log"
        benchmark:
            "log/multiqc/multiqc.tsv"
        container:
            "envs/hand_sandbox.simg"
        shell:
            "which multiqc &> {log};"
            "rm -rf fastqc/multiqc_data && multiqc fastqc/details -o fastqc &>> {log}"
else:
    rule MultiQC:
        input:
            r1=expand("fastqc/details/{sample}_fastqc.html", sample=SAMPLES),
            r2=expand("fastqc/details/{sample}_fastqc.html", sample=SAMPLES), # trick snakemake to skip r2
        output:
            "fastqc/multiqc_report.html"
        priority:
            10
        resources:
            mem_mb=lambda wildcards, attempt: attempt * 1000   
        threads:
            1
        log:
            "log/multiqc/multiqc.log"
        benchmark:
            "log/multiqc/multiqc.tsv"
        container:
            "envs/hand_sandbox.simg"
        shell:
            "which multiqc &> {log};"
            "rm -rf fastqc/multiqc_data && multiqc fastqc/details -o fastqc &>> {log}"


rule star_idx:
    input:
        fa=GENOME,
        gtf=GTF,
    output:
        INDEX+"/SAindex"
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 3200   
    threads:
        12
    log:
        "log/star_idx/star_idx.log"
    benchmark:
        "log/star_idx/star_idx.tsv"
    #container:
    #    "envs/hand_sandbox.simg" # 2.7.1 does not work for small fa, also too old index version
    envmodules:
        "star/2.7.5a"
    shell:
        """
        #mkdir -p {INDEX}
        whoami > {log};
        which STAR >> {log};

        STAR --runThreadN {threads} \
        --runMode genomeGenerate \
        --genomeDir {INDEX} \
        --genomeFastaFiles {input.fa} \
        --sjdbGTFfile {input.gtf} &>> {log}
        """


# rule STAR_Align
if config['PAIR_END']:
    rule STAR_Align:
        input:
            index=INDEX+"/SAindex",
            gtf=GTF,
            r1="fastq/{sample}.R1.fastq.gz",
            r2="fastq/{sample}.R2.fastq.gz"
        output:
            temp("mapped_reads/{sample}.bam")
        resources:
            mem_mb=lambda wildcards, attempt: attempt * 3000 
        params:
            reads="fastq/{sample}.R1.fastq.gz fastq/{sample}.R2.fastq.gz"
        threads:
            12
        log:
            "log/mapped_reads/{sample}.star.log"
        benchmark:
            "log/mapped_reads/{sample}.star.tsv"
        #container:
        #    "envs/hand_sandbox.simg" # 2.7.1 does not work for small fa, also too old index version
        envmodules:
            "star/2.7.5a"
        shell:
            """
            which STAR > {log};
            STAR --runThreadN {threads} \
            --genomeDir {INDEX} \
            --sjdbGTFfile {input.gtf} \
            --readFilesCommand zcat \
            --readFilesIn {params.reads} \
            --outFileNamePrefix mapped_reads/{wildcards.sample}. \
            --outFilterType BySJout \
            --outMultimapperOrder Random \
            --outFilterMultimapNmax 200 \
            --alignSJoverhangMin 8 \
            --alignSJDBoverhangMin 3 \
            --outFilterMismatchNmax 999 \
            --outFilterMismatchNoverReadLmax 0.05 \
            --alignIntronMin 20 \
            --alignIntronMax 1000000 \
            --alignMatesGapMax 1000000 \
            --outFilterIntronMotifs RemoveNoncanonicalUnannotated \
            --outSAMstrandField None \
            --outSAMtype BAM Unsorted \
            --quantMode GeneCounts \
            --outReadsUnmapped Fastx \
            >> {log} 2>&1

            mkdir -p bam_qc/
            mkdir -p bam_qc/STAR_summary/
            cp mapped_reads/{wildcards.sample}.Log.final.out bam_qc/STAR_summary/

            mv mapped_reads/{wildcards.sample}.Aligned.out.bam mapped_reads/{wildcards.sample}.bam
            pigz -p {threads} -f mapped_reads/{wildcards.sample}.Unmapped.out.mate*
            """
else:
    rule STAR_Align:
        input:
            index=INDEX+"/SAindex",
            gtf=GTF,
            r1="fastq/{sample}.fastq.gz",
        output:
            temp("mapped_reads/{sample}.bam")
        resources:
            mem_mb=lambda wildcards, attempt: attempt * 3000   ,
        params:
            reads="fastq/{sample}.fastq.gz",
        threads:
            12
        log:
            "log/mapped_reads/{sample}.star.log"
        benchmark:
            "log/mapped_reads/{sample}.star.tsv"
        #container:
        #    "envs/hand_sandbox.simg" # 2.7.1 does not work for small fa, also too old index version
        envmodules:
            "star/2.7.5a"
        shell:
            """
            which STAR > {log};
            STAR --runThreadN {threads} \
            --genomeDir {INDEX} \
            --sjdbGTFfile {input.gtf} \
            --readFilesCommand zcat \
            --readFilesIn {params.reads} \
            --outFileNamePrefix mapped_reads/{wildcards.sample}. \
            --outFilterType BySJout \
            --outMultimapperOrder Random \
            --outFilterMultimapNmax 200 \
            --alignSJoverhangMin 8 \
            --alignSJDBoverhangMin 3 \
            --outFilterMismatchNmax 999 \
            --outFilterMismatchNoverReadLmax 0.05 \
            --alignIntronMin 20 \
            --alignIntronMax 1000000 \
            --outFilterIntronMotifs RemoveNoncanonicalUnannotated \
            --outSAMstrandField None \
            --outSAMtype BAM Unsorted \
            --quantMode GeneCounts \
            --outReadsUnmapped Fastx \
            >> {log} 2>&1

            mv mapped_reads/{wildcards.sample}.Aligned.out.bam mapped_reads/{wildcards.sample}.bam
            pigz -p {threads} -f mapped_reads/{wildcards.sample}.Unmapped.out.mate*
            """


rule samtools_sort:
    input:
        "mapped_reads/{sample}.bam"
    output:
        "sorted_reads/{sample}.bam"
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 4000 
    threads:
        4
    container:
        "envs/hand_sandbox.simg" # 2.7.1 does not work for small fa, also too old index version
    envmodules:
        "samtools/1.9"
    log:
        "log/samtools_sort/{sample}.sort.log"
    benchmark:
        "log/samtools_sort/{sample}.sort.tsv"
    shell:
        "which samtools &> {log};"
        "samtools sort -@ {threads} -m 3G {input} -o sorted_reads/{wildcards.sample}.bam &>> {log}"


rule samtools_index:
    input:
        "sorted_reads/{sample}.bam"
    output:
        "sorted_reads/{sample}.bam.bai"
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 2000
    threads:
        1
    log:
        "log/samtools_index/{sample}.index.log"
    benchmark:
        "log/samtools_index/{sample}.index.tsv"
    container:
        "envs/hand_sandbox.simg" 
    envmodules:
        "samtools/1.9"
    shell:
        "which samtools &> {log};"
        "samtools index -@ {threads} {input} &>> {log}"


rule bam_qc:
    """
    with samtools
    """
    input:
        bam="sorted_reads/{sample}.bam",
        bai="sorted_reads/{sample}.bam.bai"
    output:
        "bam_qc/stats/{sample}.stats.txt",
        "bam_qc/idxstats/{sample}.idxstats.txt"
    container:
        "envs/hand_sandbox.simg" # not necessary after samstat disabled
    priority:
        0
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 2000
    threads:
        4
    log:
        idxstats="log/bam_qc/idxstats/{sample}.idxstats.log",
        stats="log/bam_qc/stats/{sample}.stats.log",
    benchmark:
        "log/bam_qc/benchmark/{sample}.bam_qc.tsv",
    shell:
        """
        mkdir -p bam_qc 
        mkdir -p bam_qc/idxstats
        mkdir -p bam_qc/stats

        samtools idxstats {input.bam} > bam_qc/idxstats/{wildcards.sample}.idxstats.txt 2> {log.idxstats} &
        samtools stats {input.bam} > bam_qc/stats/{wildcards.sample}.stats.txt 2> {log.stats} &

        wait
        """


# rule QoRTs
if config['PAIR_END']:
    rule QoRTs:
        input:
            bam="sorted_reads/{sample}.bam",
            gtf=config['GTF']
        output:
            "bam_qc/QoRTs/{sample}/QC.QORTS_COMPLETED_OK"
        resources:
            mem_mb=lambda wildcards, attempt: attempt * 16500
        threads:
            1
        log:
            "log/QoRTs/QoRTs.{sample}.log"
        benchmark:
            "log/QoRTs/QoRTs.{sample}.tsv"
        #container:
        #    "envs/hand_sandbox.simg" # file access issue
        shell:
            """
            mkdir -p bam_qc/
            mkdir -p bam_qc/QoRTs/
            which java &> {log}
            java -Xmx16G -jar envs/hartleys-QoRTs-099881f/QoRTs.jar QC \
            {input.bam} {input.gtf} bam_qc/QoRTs/{wildcards.sample}/ &>> {log}
            """
else:
    rule QoRTs:
        input:
            bam="sorted_reads/{sample}.bam",
            gtf=config['GTF']
        output:
            "bam_qc/QoRTs/{sample}/QC.QORTS_COMPLETED_OK"
        resources:
            mem_mb=lambda wildcards, attempt: attempt * 16500
        threads:
            1
        log:
            "log/QoRTs/QoRTs.{sample}.log"
        benchmark:
            "log/QoRTs/QoRTs.{sample}.tsv"
        #container:
        #    "envs/hand_sandbox.simg" # file access issue
        shell:
            """
            mkdir -p bam_qc/
            mkdir -p bam_qc/QoRTs/
            which java &> {log}
            java -Xmx16G -jar envs/hartleys-QoRTs-099881f/QoRTs.jar QC --singleEnded \
            {input.bam} {input.gtf} bam_qc/QoRTs/{wildcards.sample}/ &>> {log}
            """


rule QoRTs_MultiPlot:
    input:
        expand("bam_qc/QoRTs/{sample}/QC.QORTS_COMPLETED_OK", sample=SAMPLES)
    output:
        "bam_qc/QoRTs_MultiPlot/plot-basic.pdf"
    container:
        "envs/hand_sandbox.simg"
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 8000   ,
    threads:
        2
    log:
        "log/QoRTs/multiplot.log"
    benchmark:
        "log/QoRTs/multiplot.tsv"
    shell:
        """
        which python && which Rscript &> {log}
        python script/meta_to_decoder.py &>> {log}
        Rscript script/QoRT.R &>> {log}# needs QoRT package in R
        mkdir -p bam_qc/QoRTs_MultiPlot/details/
        mv bam_qc/QoRTs_MultiPlot/plot-sample* bam_qc/QoRTs_MultiPlot/details/
        """


rule bamCoverage:
    # osr
    input:
        bam="sorted_reads/{sample}.bam",
        bai="sorted_reads/{sample}.bam.bai"
    output:
        "bigWig/{sample}.{mode}.cpm.bw"
    threads:
        4
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 4000,
    priority:
        0
    log:
        "log/bamCoverage/{sample}.{mode}.bamCoverage.log"
    benchmark:
        "log/bamCoverage/{sample}.{mode}.bamCoverage.tsv"
    # not installed in hand_sandbox yet
    run:
        if {wildcards.mode} == "strict":
            shell("""
            echo '{output}'
            which bamCoverage &> {log}
            bamCoverage --bam {input.bam} \
            -o  {output} \
            --numberOfProcessors {threads} \
            --outFileFormat bigwig --normalizeUsing CPM --binSize 10 \
            --minMappingQuality 20 &>> {log}
            """)
        else:
            shell("""
            echo '{output}'
            which bamCoverage &> {log}
            bamCoverage --bam {input.bam} \
            -o  {output} \
            --numberOfProcessors {threads} \
            --outFileFormat bigwig --normalizeUsing CPM --binSize 10  &>> {log}
            """)



rule feature_count:
    input:
        bams=expand("mapped_reads/{sample}.bam", sample=SAMPLES), # sort by name -> faster
        gtf=GTF
    output:
        count="feature_count/counts.s{strand}.strict.txt" if config["MODE"] == "strict" else "feature_count/counts.s{strand}.liberal.txt",
        summary="feature_count/counts.s{strand}.strict.txt.summary" if config["MODE"] == "strict" else "feature_count/counts.s{strand}.liberal.txt.summary"
    priority:
        100
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 4000,
    threads:
        4
    log:
        "log/feature_count/counts.s{strand}.log"
    benchmark:
        "log/feature_count/benchmark.s{strand}.tsv"
    run:
        if config["PAIR_END"]:
            if config["MODE"] == "strict":
                shell("""
                which featureCounts &> {log}
                featureCounts -a {input.gtf} -o {output.count} \
                -T {threads} \
                -g gene_id -Q 20 --minOverlap 1 --fracOverlap 0 \
                -p -B -C  \
                -s {wildcards.strand} \
                {input.bams} &>> {log}
                """)
            elif config["MODE"] == "liberal":
                shell("""
                which featureCounts &> {log}
                featureCounts -a {input.gtf} -o {output.count} \
                -T {threads} \
                -g gene_id -M --primary -Q 0 --minOverlap 1 --fracOverlap 0 \
                -p \
                -s {wildcards.strand} \
                {input.bams} &>> {log}
                """)
            else:
                print ("MODE err, not liberal nor strict")
        else:
            if config["MODE"] == "strict":
                shell("""
                which featureCounts &> {log}
                featureCounts -a {input.gtf} -o {output.count} \
                -T {threads} \
                -g gene_id -Q 20 --minOverlap 1 --fracOverlap 0 \
                -s {wildcards.strand} \
                {input.bams} &>> {log}
                """)
            elif config["MODE"] == "liberal":
                shell("""
                which featureCounts &> {log}
                featureCounts -a {input.gtf} -o {output.count} \
                -T {threads} \
                -g gene_id -M --primary -Q 0 --minOverlap 1 --fracOverlap 0 \
                -s {wildcards.strand} \
                {input.bams} &>> {log}
                """)
            else:
                print ("MODE err, not liberal nor strict")
            # -p: count Fragments rather than reads for Paired-end reads (remove this for SE data)
            # -C: exclude chimeric (most times, for cancer maybe not include this)
            # -d 50, -D 1000: including PE reads with fragment length in between, which is better than default 50-600
            # -T: num threads
            # -s: strand info, very important; use $i to perform all three possibilities, pick the correct one after counting
            # -Q: min MAPQ, if MAPQ from star, we need to be careful, because star ignores PE information, we might need to add addional step to rescue PE info. (https://github.com/alexdobin/STAR/issues/615)
            # -M: count multiple-mapping reads, based on NH, not useful for RNA-seq, may create misleading summary, by counting multi-mapping reads several times
            # -B: Only count read pairs that have both ends aligned.
            # --fracOverlap 0.2: 20% of read length
            # -–minOverlap 2: 2bp
            # Notes:
            # liberal overlap settings (--minOverlap 1 --fracOverlap 0) will give you more counted reads
            # samtools sorted bams are smaller, and faster to be counted, compared to unsorted bams/star sorted bams
            # star sorted bams are slow to count => use samtools sorted reads, delete star bam (set as temp)


rule strand_detection:
    input:
        expand("feature_count/counts.s{strand}.strict.txt.summary", strand=STRAND) 
        if config["MODE"] == "strict" 
        else expand("feature_count/counts.s{strand}.liberal.txt.summary", strand=STRAND)
    output:
        "feature_count/counts.strict.txt" if config["MODE"] == "strict" else  "feature_count/counts.liberal.txt",
        "meta/strandness.detected.txt"
    threads:
        1
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 1000
    log:
        "log/strand_detection.log"
    benchmark:
        "log/strand_detection.tsv"
    script:
        "script/strandness_detection.py" # todo: improvement needed



# rule SalmonTE_prep
if config['PAIR_END']:
    rule SalmonTE_prep:
        input:
            r1=expand("fastq/{sample}.R1.fastq.gz", sample=SAMPLES),
            r2=expand("fastq/{sample}.R2.fastq.gz", sample=SAMPLES)    
        output:
            r1=expand("fastq_salmon/{sample}_1.fastq.gz", sample=SAMPLES),
            r2=expand("fastq_salmon/{sample}_2.fastq.gz", sample=SAMPLES)
        resources:
            mem_mb=lambda wildcards, attempt: attempt * 1000
        threads:
            1
        shell:
            """
            rm -rf fastq_salmon
            mkdir -p fastq_salmon
            cd fastq_salmon/
            for f in ../fastq/*gz;do ln -s $f;done
            rename .R1 _1 *
            rename .R2 _2 *
            cd ..
            """


# rule SalmonTE
if config['PAIR_END']:
    rule SalmonTE:
        # only start from Fastq
        input:
            r1=expand("fastq_salmon/{sample}_1.fastq.gz", sample=SAMPLES),
            r2=expand("fastq_salmon/{sample}_2.fastq.gz", sample=SAMPLES)
        output:
            "SalmonTE_output/EXPR.csv"
        resources:
            mem_mb=lambda wildcards, attempt: attempt * 1000,
        params:
            ref=config['TE_REFERENCE']
        threads:
            12
        log:
            'log/SalmonTE/salmonte_count.log'
        benchmark:
            'log/SalmonTE/salmonte_count.tsv'
        shell:
            """
            which python &> {log}
            python envs/SalmonTE/SalmonTE.py quant \
            --reference={params.ref} --exprtype=count \
            --num_threads={threads} \
            {input.r1} {input.r2} >> {log} 2>&1
            """
else:
    rule SalmonTE:
        # only start from Fastq
        input:
            r1=expand("fastq/{sample}.fastq.gz", sample=SAMPLES),
        output:
            "SalmonTE_output/EXPR.csv"
        resources:
            mem_mb=lambda wildcards, attempt: attempt * 1000,
        params:
            ref=config['TE_REFERENCE']
        threads:
            12
        log:
            'log/SalmonTE/salmonte_count.se.log'
        benchmark:
            'log/SalmonTE/salmonte_count.se.tsv'
        shell:
            """
            which python &> {log}
            python envs/SalmonTE/SalmonTE.py quant \
            --reference={params.ref} --exprtype=count \
            --num_threads={threads} \
            {input.r1} >> {log} 2>&1
            """

rule Merge_TE_and_Gene:
    input:
        gene="feature_count/counts.strict.txt"
            if config['MODE'] == 'strict'
            else "feature_count/counts.liberal.txt",
        te="SalmonTE_output/EXPR.csv"
    output:
        "feature_count/TE_included.txt"
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 16000,
    threads:
        1
    log:
        "log/merge_te_and_gene/merge_te_and_gene.log"
    benchmark:
        "log/merge_te_and_gene/merge_te_and_gene.tsv"
    shell:
        """
        python script/merge_featureCount_and_SalmonTE.py \
        {input.gene} {input.te} {output} > {log} 2>&1
        """


def DESeq2_input(config):
    if config["START"] in ["FASTQ", "BAM"]:
        if config["MODE"] == "strict":
            if config["TE_ANALYSIS"] and config["START"] == "FASTQ":
                return("feature_count/TE_included.txt")
            else:
                return("feature_count/counts.strict.txt")
        elif config["MODE"] == "liberal":
            if config["TE_ANALYSIS"] and config["START"] == "FASTQ":
                return("feature_count/TE_included.txt")
            else:
                return("feature_count/counts.liberal.txt")
        else:
            raise Exception("START config not valid")
    elif config["START"] in ["COUNT"]:
        if config["COUNT_FILE"].endswith("txt"):
            return(config["COUNT_FILE"])
        elif config["COUNT_FILE"].endswith("xlsx"):
            df = pd.read_excel(config["COUNT_FILE"])
            df.to_csv("meta/COUNT.xlsx.txt", sep="\t", index=False, header=True)
            return("meta/COUNT.xlsx.txt")
        else:
            raise NameError('COUNT file not txt nor xlsx format, Exit')
    else:
        return("placeholder" )


rule DESeq2:
    input:
        DESeq2_input(config)
    output:
        "DESeq2/DESeq2.html",
        expand("DESeq2/rnk/{contrast}.rnk", contrast=CONTRASTS_DE)
    container:
        "envs/hand_sandbox.simg"
    # conda:
    #     "envs/r.yaml"
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 2000,
    params:
        rmd="'script/DESeq2.Rmd'",
        fdr=MAX_FDR,
        lfc=MIN_LFC,
        anno_tab=ANNO_TAB,
        meta=config["META"],
        cont=config["CONTRAST_DE"],
        o="'../DESeq2/DESeq2.html'"
    priority: 
        100
    threads:
        2
    log:
        "log/DESeq2/DESeq2.log"
    benchmark:
        "log/DESeq2/DESeq2.tsv"
    shell:
        #Rscript -e rmarkdown::render({params.rmd})
        'mkdir -p DESeq2; ' 
        'cp script/DESeq2.Rmd DESeq2; '
        'Rscript -e "rmarkdown::render({params.rmd}, \
        params=list(max_fdr={params.fdr}, min_lfc={params.lfc}, \
        countFile=\'../{input}\', \
        annoFile=\'{params.anno_tab}\', \
        metaFile=\'../{params.meta}\', \
        contrastFile=\'../{params.cont}\'           ), \
        output_file={params.o})" > {log} 2>&1'


rule GSEA:
    input: 
        deseq2= "Workflow_DAG.all.svg" # dummy assume existing
            if config["START"] == "RNK" 
            else "DESeq2/DESeq2.html" , # to keep ordering
        rnk="meta/{contrast}"  # contrast just a name, alias for rnk
            if config["START"] == "RNK" 
            else "DESeq2/rnk/{contrast}.rnk",
        db="envs/gsea_db/{db}"
    output:
        touch("log/gsea/{contrast}/{db}.finished"),  # todo: premature killed jobs create junk sub folders, confusing to users
        "gsea/{contrast}/{db}.GseaPreranked/index.html"
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 12000,
    params:
        svg=config["GSEA_PLOT_SVG"],
        nplot=config["GSEA_NPLOTS"],
        label_db=lambda wildcards: wildcards["db"][:-12]
    threads:
        1
    log:
        "log/gsea/{contrast}.{db}.log"
    benchmark:
        "log/gsea/{contrast}.{db}.tsv"
    envmodules:
        "openjdk/11+28"
    # run:
    #     print('test')
    shell:
        """
        echo {params.label_db} > {log}
        
        mkdir -p gsea && mkdir -p gsea/{wildcards.contrast} &>> {log}
        rm -rf gsea/{wildcards.contrast}/{wildcards.db}.GseaPreranked.*/  # avoid confusion
        rm -rf gsea/{wildcards.contrast}/{wildcards.db}.GseaPreranked/  # avoid dir structure mistake

        python script/capslock.py {input.rnk} &>> {log}
        
        which java &>> {log}

        envs/GSEA_4.0.3/gsea-cli.sh GSEAPreranked \
        -gmx {input.db} -rnk {input.rnk} -rpt_label {wildcards.db} \
        -norm meandiv -nperm 1000  -scoring_scheme classic \
        -create_svgs {params.svg} -make_sets true  -rnd_seed timestamp -zip_report false \
        -set_max 15000 -set_min 15 \
        -plot_top_x {params.nplot} -out ./gsea/{wildcards.contrast} &>> {log}

        mv gsea/{wildcards.contrast}/{wildcards.db}.GseaPreranked.*/ gsea/{wildcards.contrast}/{wildcards.db}.GseaPreranked/   # if gsea fails, might still have problems, not sure
        cp envs/GSEA_MSigDB_ReadMe.html gsea/
        cp envs/GSEA_MSigDB_ReadMe.html gsea_compressed/
        """

# todo: if gsea/contrast/err_{db}, touch gsea/contrast/db.finished

rule GSEA_compression:
    input:
        "log/gsea/{contrast}/{db}.finished"
    output:
        "gsea_compressed/{contrast}/{db}.GseaPreranked.tar.gz"
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 1000,
    threads:
        4
    log:
        "log/gsea/{contrast}.{db}.compression.log"
    benchmark:
        "log/gsea/{contrast}.{db}.compression.tsv"
    shell:
        "mkdir -p gsea_compressed && \
        [ -d gsea/{wildcards.contrast}/{wildcards.db}.GseaPreranked/ ] && \
        tar cf - gsea/{wildcards.contrast}/{wildcards.db}.GseaPreranked/  | pigz -p {threads} -f > gsea_compressed/{wildcards.contrast}/{wildcards.db}.GseaPreranked.tar.gz \
        2> {log}"


rule read_length_detection_bam:
    input:
        expand("bam_qc/stats/{sample}.stats.txt", sample=SAMPLES)
            if config['START'] in ["FASTQ", "BAM"]
            else "BAM file not provided. AND can't be generated from FASTQ, because FASTQ not provided"
    output:
        "meta/read_length.txt"
    resources:
        mem_mb="1000",
    threads:
        1
    log:
        "log/read_length_detection_bam/read_length_detection_bam.log"
    benchmark:
        "log/read_length_detection_bam/read_length_detection_bam.tsv"
    run:
        import re
        import statistics
        lengths = []
        for f in input:
            p = re.compile("average length:\s*(\d*)")
            for i, line in enumerate(open(f, "r")):
                for match in re.finditer(p, line):
                    lengths.append(match.group(1))
        lengths = list(map(int, lengths))
        print("read lengths detected from BAM:", lengths,"\n")
        if(len(set(lengths)) < 1):
            sys.exit("read lengths detection from BAM failed")
        if(len(set(lengths)) > 1):
            sys.exit("Not all fastq files have the same length")
        
        median_length = int(statistics.median(lengths))
        print(output[0])
        with open(output[0], "w") as out:
            out.write(str(median_length))

def get_strandness (x):
    try:
        with open("meta/strandness.detected.txt", "r") as file:
            txt = file.readline()
        #print("meta/strandness.detected.txt:", txt)
        p = re.compile("counts.s(.)\.[liberal|strict]")
        res = p.search(txt)
        #print("strand code:", res.group(1))
        if res.group(1) is None:
            sys.exit("strandness detection wrong")
        strand = (config['RMATS_STRANDNESS'][int(res.group(1))])
        return (strand)
    except FileNotFoundError:
        print("meta/strandness.detected.txt will be found in real run, not in dry run")
        return (None)


def read_length(x):
    try:
        with open("meta/read_length.txt", "r") as file:
            txt = file.readline()
        #print("read length", int(txt))
        return (int(txt))
    except FileNotFoundError:
        print("meta/read_length.txt not found in dry run, will be found in real run")
        return (None)


# rule rMATS
# conda: snakemake --use-conda -p -j 5 rMATS/bam_test_genecode/ASEvents/fromGTF.A3SS.txt
# python envs/rMATS.3.2.5/RNASeq-MATS.py -b1 sorted_reads/KO_D8_1_S6.bam,sorted_reads/KO_D8_2_S12.bam,sorted_reads/WT_D8_1_S5.bam,sorted_reads/WT_D8_2_S11.bam -b2  sorted_reads/KO_D0_1_S2.bam,sorted_reads/KO_D0_2_S8.bam,sorted_reads/WT_D0_1_S1.bam,sorted_reads/WT_D0_2_S7.bam -gtf genome/mm10_chr19/genecode.vM21.chr19.gtf -o rMATS -t paired -len 100 -c 0.0001 -analysis paired -libType fr-unstranded
# v 3.5 have no pvalues, must use 4.0
# "singularity exec ~/singularity/hand_sandbox/ \
# /opt/anaconda3/envs/rmat/bin/python /opt/bin/rMATS.4.0.2/rMATS-turbo-Linux-UCS4/rmats.py \
# --b1 b1.txt --b2 b2.txt \
# --gtf Homo_sapiens.GRCh37.75.gtf \
# --od rMATS  -t {params.type}} --readLength 100 --cstat 0.0001 --libType fr-unstranded"

rule rMATS:
    #todo: rMATS4.0(sandbox failed because of apt dependency, still got partial results)
    #todo: update to docker: simple to install, faster, recommended on website
    input:
        b1=lambda wildcards: B1S[int(wildcards['ascn'])],
        b2=lambda wildcards: B2S[int(wildcards['ascn'])],
        gtf=GTF, 
        length_file="meta/read_length.txt" , 
        strand_file="meta/strandness.detected.txt",
    output:
        "rMATS.{ascn}/output/Results_JunctionCountsBased/RI.MATS.JC.txt"
    envmodules:
        "rMATS/4.1.0 "
        "gcc/8.1.0 " # must have space
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 4000,  # todo reduce
    threads:
        4
    params:
        b1=lambda wildcards: B1[int(wildcards['ascn'])], 
        b2=lambda wildcards: B2[int(wildcards['ascn'])], 
        type="paired" if config['PAIR_END'] else "single", 
        analysis="P" if config['PAIR_END'] else "U", 
        length=read_length, 
        strandness=get_strandness,
        MAX_FDR=MAX_FDR
    log:
        "log/rMATS/rMATS.{ascn}.log"
    benchmark:
        "log/rMATS/rMATS.{ascn}.tsv"
    shell:
        """
        rm -rf rMATS.{wildcards.ascn}
        mkdir -p rMATS.{wildcards.ascn}/
        mkdir -p rMATS.{wildcards.ascn}/params/


        echo {params.b1} > rMATS.{wildcards.ascn}/params/b1.txt
        echo {params.b2} > rMATS.{wildcards.ascn}/params/b2.txt

        which python &> {log}
        which rmats.py &>> {log}

        # todo: remove hardcode path
        # todo: --paired-stats
        python /share/pkg/rMATS/4.1.0/rmats.py  \
        --b1 rMATS.{wildcards.ascn}/params/b1.txt \
        --b2  rMATS.{wildcards.ascn}/params/b2.txt \
        --gtf {input.gtf} \
        -t {params.type} \
        --readLength {params.length} \
        --variable-read-length \
        --libType {params.strandness} \
        --nthread {threads} \
        --tstat {threads} \
        --cstat 0.2 \
        --od rMATS.{wildcards.ascn}/output/ \
        --tmp rMATS.{wildcards.ascn}/tmp/  &>> {log}

        cp envs/rMATS_ReadMe.html rMATS.{wildcards.ascn}/output/

        rm -rf rMATS.{wildcards.ascn}/tmp/

        mkdir -p rMATS.{wildcards.ascn}/output/Results_JunctionCountsBased/
        mkdir -p rMATS.{wildcards.ascn}/output/Results_JunctionCountsAndExonCountsBased/
        mkdir -p rMATS.{wildcards.ascn}/output/fromGTF/
        mv rMATS.{wildcards.ascn}/output/*JCEC.txt rMATS.{wildcards.ascn}/output/Results_JunctionCountsAndExonCountsBased/
        mv rMATS.{wildcards.ascn}/output/*JC.txt rMATS.{wildcards.ascn}/output/Results_JunctionCountsBased/
        mv rMATS.{wildcards.ascn}/output/fromGTF*txt  rMATS.{wildcards.ascn}/output/fromGTF/
        rm -f rMATS.{wildcards.ascn}/output/*raw.input*
        """


DEXSeq_GFF = config['GTF']+".dexseq.gff"

rule DEXSeq_Prep_GFF:
    input:
        config['GTF']
    output:
        DEXSeq_GFF
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 4000
    threads:
        1
    log:
        "log/DEXSeq/prep_gff.log"
    benchmark:
        "log/DEXSeq/prep_gff.tsv"
    shell:
        "python script/dexseq_prepare_annotation.py -r no {input} {output} &> {log}"


def get_strandness_for_dexseq (x):
    book = {0 : '-s no', 1 : '-s yes', 2 : '-s reverse'}
    try:
        with open("meta/strandness.detected.txt", "r") as file:
            txt = file.readline()
        #print("meta/strandness.detected.txt:", txt)
        p = re.compile("counts.s(.)\.[liberal|strict]")
        res = p.search(txt)
        #print("strand code:", res.group(1))
        if res.group(1) is None:
            sys.exit("strandness detection wrong")
        strand = book[int(res.group(1))]
        return (strand)
    except FileNotFoundError:
        print("meta/strandness.detected.txt will be found in real run, not in dry run")
        return (None)

rule prep_count:
    input:
        bam="sorted_reads/{sample}.bam",
        strandFile="meta/strandness.detected.txt", 
        gff=DEXSeq_GFF
    output:
        "DEXSeq_count/{sample}_count.txt"
    params:
        strand=get_strandness_for_dexseq,
        readType=('-p yes -r pos'
            if config['PAIR_END']
            else ' '), 
        MAPQ=10
    resources:
         mem_mb=lambda wildcards, attempt: attempt * 16000
    log:
        "log/DEXSeq/prep_count.{sample}.log"
    benchmark:
        "log/DEXSeq/prep_count.{sample}.tsv"
    #conda:
    #    "envs/dexseq.yaml"
    shell:
        "python script/dexseq_count.py -f bam -a {params.MAPQ} {params.readType} {params.strand} {DEXSeq_GFF} {input.bam} {output}"  
        # todo: maybe -r name (sort by name) is faster


rule DEXSeq:
    input:
        count_files = expand("DEXSeq_count/{sample}_count.txt", sample = SAMPLES),
        meta=config["META"],
        contrast=config["CONTRAST_DE"],
        gffFile=config['GTF']+".dexseq.gff",
        strandness="meta/strandness.detected.txt"
    output:
        #touch("DEXSeq/DEXSeq.finished"),
        expand("DEXSeq/DEXSeq_{contrast}.xlsx", contrast=CONTRASTS_DE_for_DEX)
    resources:
         mem_mb=lambda wildcards, attempt: attempt * 8000
    params:
        rmd="'script/dexseq.r'",
        annoFile=config["ANNO_TAB"],
        # fdr=MAX_FDR, # todo: add MAX_FDR
        #lfc=MIN_LFC,
        #anno_tab=ANNO_TAB,
    # envmodules:
    #    "R/3.5.2_gcc8.1.0" # only works for Rui, after he installed packages
    #container:
    #    "envs/hand_sandbox.simg"
    conda:
        "envs/dexseq.yaml"
    threads:
        4
    log:
        "log/DEXSeq/DEXSeq.log"
    benchmark:
        "log/DEXSeq/DEXSeq.tsv"
    shell:
        "mkdir -p DEXSeq \n"
        "which Rscript &> {log} \n"
        "cp script/dexseq.r DEXSeq/ \n"
        "Rscript script/dexseq.r \
        {input.meta} {input.contrast} {input.gffFile} {params.annoFile} &>> {log}"

rule genome_faidx:
    input:
        config['GENOME']
    output:
        config['GENOME']+'.fai'
    resources:
        mem_mb=lambda wildcards, attempt: attempt *4000
    threads:
        1
    log:
        "log/genome_faidx/faidx.log"
    shell:
        "samtools faidx {input}  &> {log}"


rule GATK_CreateSequenceDictionary:
    input:
        config['GENOME'],
    output:
        re.sub( "fa$", "dict", config['GENOME'])
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 16000
    envmodules:
        "java/1.8.0_171"
    threads:
        1
    log:
        "log/GATK_CreateSequenceDictionary/GATK_CreateSequenceDictionary.log"
    benchmark:
        "log/GATK_CreateSequenceDictionary/GATK_CreateSequenceDictionary.tsv"
    shell:
        "./envs/gatk-4.1.8.1/gatk CreateSequenceDictionary -R {input} &> {log}"


rule GATK_ASEReadCounter:
    input:
        genome=config['GENOME'],
        genome_idx=config['GENOME']+'.fai',
        genome_ref=re.sub( "fa$", "dict", config['GENOME']),
        bam="sorted_reads/{sample}.bam",
        vcf=config['VCF']
    output:
        table="GATK_ASEReadCounter/{sample}.table"
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 16000
    envmodules:
        "java/1.8.0_171"
    threads:
        1
    log:
        "log/GATK_ASEReadCounter/{sample}.log"
    benchmark:
        "log/GATK_ASEReadCounter/{sample}.tsv"
    shell:
        "./envs/gatk-4.1.8.1/gatk ASEReadCounter -R {input.genome} -I {input.bam} -V {input.vcf} -O {output} \
        --min-depth-of-non-filtered-base 10 --min-mapping-quality 15 --min-base-quality 20 \
        &> {log}"  # todo: more thoughts on detailed parameters, e.g.    -drf DuplicateRead -U ALLOW_N_CIGAR_READS
        

rule create_dag:
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 1000,
    threads:
        1
    output:
        "Workflow_DAG.all.svg"
    log:
        "log/create_dag/Workflow_DAG.all.svg.log"
    envmodules:
        "graphviz/2.26.0"
    shell:
        "snakemake --dag targets > dag 2> {log};"
        "which dot &>> {log};"
        "cat dag|dot -Tsvg > {output} 2>> {log}"


# rule organize_results:
#     input:
#         mapping=expand("mapped_reads/{sample}.bam", sample=config['SAMPLES']),
#     output:
#         touch("log/oranize_results.finished")
#     params:
#         mem_mb="1000"
#     log:
#         "log/organize_results.log"
#     shell:
#         "gzip -f mapped_reads/*.Unmapped.out.mate*"
# 

rule reset:
    shell:
        """
        echo 'deleting files..'
        rm -rf fastqc/ bam_qc/ mapped_reads/ sorted_reads/ bam_qc/ bigWig/ \
        feature_count/ fastq_salmon SalmonTE_output/ DESeq2/ gsea/ gsea_compressed/ \
        GATK_ASEReadCounter/ DEXSeq_count/  DEXSeq/ rMATS.*/ \
        _STARgenome _STARtmp \
        lsf.log Log.out nohup.out report.log report.html  dag Workflow_DAG.all.svg \
        meta/strandness.detected.txt  meta/decoder.txt meta/read_length.txt
        echo 'unlocking dir..'
        snakemake -j 1 --unlock
        """

