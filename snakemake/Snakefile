configfile: "config.yaml"
#container: "envs/hand_sandbox.simg"  # only effective when --use-singularity
# removed gloabal singularity so that can use --use-singularity and --use-conda at the same time

#localrules: targets, create_dag


SAMPLES=config['SAMPLES']
GENOME=config["GENOME"]
INDEX=config["INDEX"]
GTF=config["GTF"]
ANNO_TAB=config["ANNO_TAB"]
STRAND=config["STRAND"]
MODE=config["MODE"]
MAX_FDR=config["MAX_FDR"]
MIN_LFC=config["MIN_LFC"]

# priority 0-100, default 0

# get CONTRASTS from meta/contrast.xlsx
import pandas as pd
import sys
import re
import os

# Get contrast name for DESeq2
# todo: fix svg
def get_contrast_fnames (fname):
    df=pd.read_excel(fname)
    CONTRASTS = []
    for j in range(df.shape[1]):
        c1 = df.iloc[0,j]
        c2 = df.iloc[1,j]
        c1 = c1.replace(" ", "")
        c2 = c2.replace(" ", "")
        c1 = re.sub(";$", "", c1)  # remove extra ;
        c2 = re.sub(";$", "", c2)
        c1 = re.sub(";", ".", c1)  # remove extra ;
        c2 = re.sub(";", ".", c2)
        CONTRASTS.append(c1 + "_vs_" + c2)
    # CONTRATS = ['KO_D8_vs_KO_D0', 'WT_D8.KO_D8_vs_WT_D0.KO_D0', 'KO_D0.KO_D2.KO_D8_vs_WT_D0.WT_D2.WT_D8']
    return (CONTRASTS)


def get_contrast_groups (fname):
    df2=pd.read_excel(fname)
    C1S = []; C2S = []
    for j in range(df2.shape[1]):
        c1 = df2.iloc[0, j].strip()
        c2 = df2.iloc[1, j].strip()
        c1 = c1.replace(" ", "")
        c2 = c2.replace(" ", "")
        c1 = re.sub(";$", "", c1)  # remove extra ;
        c2 = re.sub(";$", "", c2)
        c1s = c1.split(";")
        c2s = c2.split(";")
        C1S.append(c1s)
        C2S.append(c2s)
    return ([C1S, C2S])


def get_dict_from_meta (fname):
    df = pd.read_excel(fname)
    d = {}
    for i in range(df.shape[0]):
        sample = df.iloc[i, 0]
        group = df.iloc[i, 1]
        if group in d:
            d[group].append(sample)
        else:
            d[group] = []
            d[group].append(sample)
    return (d)


def swap_list_element(l, d):
    return [swap_list_element(x, d) if isinstance(x, list) else d.get(x, x) for x in l]
    
def flatten(l):
    flat_list = [item for sublist in l for item in sublist]
    return (flat_list)

def collapseL2(L3):
    L2 = []
    for l in L3:
        L2.append(flatten(l))
    return (L2)

def s2b(S):
    K = flatten(S)
    V = ["sorted_reads/"+x+".bam" for x in K]
    return (dict(zip(K, V)))

def G2B_workflow(G, g2s):
    S = swap_list_element(G, g2s)
    S = collapseL2(S)
    s2b_dict = s2b(S)
    BS = swap_list_element(S, s2b_dict)
    B = [",".join(x) for x in BS]
    return (BS, B)



# for DESeq2
if config['START'] in ["FASTQ", "BAM", "COUNT"]:
    CONTRASTS_DE = get_contrast_fnames(config['CONTRAST_DE'])
    #print("CONTRASTS_DE:\n", CONTRASTS_DE)
else:
    CONTRASTS_DE = ["placeholder"]

# for rMATS
if config['START'] in ["FASTQ", "BAM"]:
    G = get_contrast_groups(config['CONTRAST_AS'])
    #print("\nGRPUPS:\n", G[0], "\nVS\n", G[1])

    g2s = get_dict_from_meta(config['META'])
    #print("\nGroup To Sample Mapping:\n", g2s)

    B1S, B1 = G2B_workflow(G[0], g2s)
    B2S, B2 = G2B_workflow(G[1], g2s)

    # print("\nrMATS -b1:\n", B1)
    # print("B1S:", B1S)
    # print("rMATS -b2:\n", B2)
    # print("B2S:", B2S)

    # Get contrast for rMATS
    df2 = pd.read_excel(config['CONTRAST_AS'])
    ASCN = df2.shape[1] # alternative splicing contrast count
    #print("ASCN", ASCN)
else:
    B1 = "placeholder.bam"
    B1S = ["placeholder"]
    B2 = B1
    B2S = B1S
    ASCN = 0



# load modules (have to use """, to keep in one block)
# - alias does not work, have to use $samstat
shell.prefix("""
            # module load star/2.5.3a
            # module load singularity/singularity-current
            # conda activate osr  # have to create conda env osr first
            #samstat="singularity exec envs/hand_sandbox samstat"
            #samtools="singularity exec envs/hand_sandbox.simg samtools"
            """)

# Requirements
# inputs in ./fastq/xxx.{fastq,fq}.gz
# named as {sample}.{R1,R2}.{fastq,fq}.gz

# SnakeMake Coding Notes:
# input don't have to be used, just for draw nice DAG
# 07/10/2019 randomized primary alignment

ruleorder: create_dag > rMATS > DESeq2 > GSEA > fastqc > bam_qc > bamCoverage > feature_count > reset > samtools_sort > samtools_index

rule targets:
    input:
        # 1. everything listed here will be produced by the pipeline
        # 2. feed {sample}
        fastqc=("fastqc/multiqc_report.html" 
            if config["START"] == "FASTQ" 
            else "Workflow_DAG.all.svg"),
        bam_qc=(expand("bam_qc/samstat/{sample}.bam.samstat.html", sample=SAMPLES) 
            if config["START"] in ["FASTQ", "BAM"]  
            else "Workflow_DAG.all.svg"),
        bamCoverage=(expand("bigWig/{sample}.{mode}.cpm.bw", sample=SAMPLES, mode=MODE) 
            if config["START"] in ["FASTQ", "BAM"]  
            else "Workflow_DAG.all.svg"),
        feature_count_strict=(expand("feature_count/counts.s{strand}.strict.txt", strand=STRAND)
            if config["START"] in ["FASTQ", "BAM"] and config["MODE"] == "strict"
            else "Workflow_DAG.all.svg"),
        feature_count_liberal=(expand("feature_count/counts.s{strand}.liberal.txt", strand=STRAND)       
            if config["START"] in ["FASTQ", "BAM"] and config["MODE"] == "liberal"
            else "Workflow_DAG.all.svg"),
        DESeq2="DESeq2/DESeq2.html"
            if config["START"] in ["FASTQ", "BAM", "COUNT"]
            else "Workflow_DAG.all.svg",
        GSEA = expand("gsea/{contrast}/{db}.finished", contrast=CONTRASTS_DE, db=config["GSEA_DBS"])
            if config["START"] in ["FASTQ", "BAM", "COUNT"]
            else expand("gsea/{contrast}/{db}.finished", contrast=config["RNKS"], db=config["GSEA_DBS"]), # contrast just a name
        rMATS=expand("rMATS.{ascn}/MATS_output/RI.MATS.ReadsOnTargetAndJunctionCounts.txt", ascn=range(ASCN))
            if config['AS_ANALYSIS'] and config["START"] in ["FASTQ", "BAM"]  # todo: estimate read length from BAM
            else "Workflow_DAG.all.svg",
        dag="Workflow_DAG.all.svg", # create DAG
    params:
        mem_mb="1000"


rule fastqc:
    # don't need input, if you agree on not checking them
    # without output, output will not be created
    input:
        r1="fastq/{sample}.R1.fastq.gz" 
            if config["PAIR_END"] 
            else "fastq/{sample}.fastq.gz",
        r2="fastq/{sample}.R2.fastq.gz" 
            if config["PAIR_END"] 
            else "fastq/{sample}.fastq.gz", # trick snakemake to skip r2
    output:
        r1="fastqc/details/{sample}.R1_fastqc.html" 
            if config["PAIR_END"] 
            else "fastqc/details/{sample}_fastqc.html",
        r2="fastqc/details/{sample}.R2_fastqc.html" 
            if config["PAIR_END"] 
            else "fastqc/details/{sample}_fastqc.html", # trick snakemake to skip r2
    priority:
        10
    params:
        mem_mb="2000"
    threads:
        1
    log:
        "log/fastqc/{sample}.log"
    benchmark:
        "log/fastqc/{sample}.tsv"
    run:
        shell("mkdir -p fastqc && mkdir -p fastqc/details")
        if config["PAIR_END"]:
            shell("fastqc -t {threads} {input} -o fastqc/details &> {log}")
        else:
            shell("fastqc -t {threads} {input.r1} -o fastqc/details &> {log}")


rule multiqc:
    input:
        r1=expand("fastqc/details/{sample}.R1_fastqc.html", sample=SAMPLES) 
            if config["PAIR_END"] 
            else expand("fastqc/details/{sample}_fastqc.html", sample=SAMPLES),
        r2=expand("fastqc/details/{sample}.R2_fastqc.html", sample=SAMPLES) 
            if config["PAIR_END"] 
            else expand("fastqc/details/{sample}_fastqc.html", sample=SAMPLES), # trick snakemake to skip r2
    priority:
        10
    params:
        mem_mb="2000"
    threads:
        1
    output:
        "fastqc/multiqc_report.html"
    log:
        "log/multiqc/multiqc.log"
    benchmark:
        "log/multiqc/multiqc.tsv"
    shell:
        "rm -rf fastqc/multiqc_data && multiqc fastqc/details -o fastqc &> {log}"


rule star_idx:
    input:
        fa=GENOME,
        gtf=GTF,
    output:
        directory(INDEX)
    params:
        mem_mb="6000"
    threads:
        6
    log:
        "log/star_idx/star_idx.log"
    benchmark:
        "log/star_idx/star_idx.tsv"
    shell:
        """
        mkdir -p {INDEX}

        STAR --runThreadN {threads} \
        --runMode genomeGenerate \
        --genomeDir {INDEX} \
        --genomeFastaFiles {input.fa} \
        --sjdbGTFfile {input.gtf} &> {log}
        """


rule star_map:
    input:
        index=INDEX,
        gtf=GTF,
        r1="fastq/{sample}.R1.fastq.gz" 
            if config["PAIR_END"] 
            else "fastq/{sample}.fastq.gz",
        r2="fastq/{sample}.R2.fastq.gz" 
            if config["PAIR_END"] 
            else "fastq/{sample}.fastq.gz", # trick snakemake to skip r2
    output:
        bam=temp("mapped_reads/{sample}.bam")
    params:
        mem_mb="3000",  # todo auto adjust based on {threads}
        reads="fastq/{sample}.R1.fastq.gz fastq/{sample}.R2.fastq.gz" if config["PAIR_END"] else "fastq/{sample}.fastq.gz",
    threads:
        12
    log:
        "log/mapped_reads/{sample}.star.log"
    benchmark:
        "log/mapped_reads/{sample}.star.tsv"
    run:
        # align; rename
        shell("""STAR --runThreadN {threads} \
        --genomeDir {INDEX} \
        --sjdbGTFfile {input.gtf} \
        --readFilesCommand zcat \
        --readFilesIn {params.reads} \
        --outFileNamePrefix mapped_reads/{wildcards.sample}. \
        --outFilterType BySJout \
        --outMultimapperOrder Random \
        --outFilterMultimapNmax 200 \
        --alignSJoverhangMin 8 \
        --alignSJDBoverhangMin 3 \
        --outFilterMismatchNmax 999 \
        --outFilterMismatchNoverReadLmax 0.05 \
        --alignIntronMin 20 \
        --alignIntronMax 1000000 \
        --alignMatesGapMax 1000000 \
        --outFilterIntronMotifs RemoveNoncanonicalUnannotated \
        --outSAMstrandField None \
        --outSAMtype BAM Unsorted \
        --quantMode GeneCounts \
        --outReadsUnmapped Fastx \
        &> {log}

        mv mapped_reads/{wildcards.sample}*.out.bam mapped_reads/{wildcards.sample}.bam
        
        pigz -p {threads} -f mapped_reads/{wildcards.sample}.Unmapped.out.mate*
        """)

        # test
        # STAR --runThreadN 4 --genomeDir genome/mm10_chr19/osr_idx/ --sjdbGTFfile genome/mm10_chr19/genecode.vM21.chr19.gtf --readFilesCommand zcat --readFilesIn fastq/KO_D0_1_S2.R1.fastq.gz fastq/KO_D0_1_S2.R2.fastq.gz 



rule samtools_sort:
    input:
        "mapped_reads/{sample}.bam"
    output:
        "sorted_reads/{sample}.bam"
    # conda:
    #   "envs/samtools.yaml"
    # group:
    #     "sort"
    params:
        mem_mb="3000"
    threads:
        2
    log:
        "log/samtools_sort/{sample}.sort.log"
    benchmark:
        "log/samtools_sort/{sample}.sort.tsv"
    shell:
        "samtools sort -@ {threads} -m 3G {input} -o sorted_reads/{wildcards.sample}.bam &> {log}"


rule samtools_index:
    input:
        "sorted_reads/{sample}.bam"
    output:
        "sorted_reads/{sample}.bam.bai"
    # conda:
    #   "envs/samtools.yaml"
    # group:
    #     "sort"
    params:
        mem_mb="3000"
    threads:
        1
    log:
        "log/samtools_index/{sample}.index.log"
    benchmark:
        "log/samtools_index/{sample}.index.tsv"
    shell:
        "samtools index -@ {threads} {input} &> {log}"


rule bam_qc:
    input:
        bam="sorted_reads/{sample}.bam",
        bai="sorted_reads/{sample}.bam.bai"
    output:
        "bam_qc/samstat/{sample}.bam.samstat.html"
    container:
        "envs/hand_sandbox.simg"
    priority:
        0
    params:
        mem_mb="4000"
    threads:
        4
    log:
        idxstats="log/bam_qc/idxstats/{sample}.idxstats.log",
        flagstat="log/bam_qc/flagstat/{sample}.flagstat.log",
        stats="log/bam_qc/stats/{sample}.stats.log",
        samstat="log/bam_qc/samstat/{sample}.samstat.log",
    benchmark:
        "log/bam_qc/benchmark/{sample}.bam_qc.tsv",
    shell:
        """
        mkdir -p bam_qc 
        mkdir -p bam_qc/idxstats
        mkdir -p bam_qc/flagstat
        mkdir -p bam_qc/stats
        mkdir -p bam_qc/samstat
        samtools idxstats {input.bam} > bam_qc/idxstats/{wildcards.sample}.idxstats.txt 2> {log.idxstats} &
        samtools flagstat {input.bam} > bam_qc/flagstat/{wildcards.sample}.flagsat.txt 2> {log.flagstat} &
        samtools stats {input.bam} > bam_qc/stats/{wildcards.sample}.stats.txt 2> {log.stats} &
        samstat {input.bam} && mv sorted_reads/{wildcards.sample}*.samstat.html bam_qc/samstat 2> {log.samstat}
        wait
        """


rule bamCoverage:
    input:
        bam="sorted_reads/{sample}.bam",
        bai="sorted_reads/{sample}.bam.bai"
    output:
        "bigWig/{sample}.{mode}.cpm.bw"
    threads:
        8
    params:
        mem_mb="4000",
        common_strict="--outFileFormat bigwig --normalizeUsing CPM --minMappingQuality 20 --binSize 10 ",
        common_liberal="--outFileFormat bigwig --normalizeUsing CPM --binSize 10 ",
    priority:
        0
    log:
        "log/bamCoverage/{sample}.{mode}.bamCoverage.log"
    benchmark:
        "log/bamCoverage/{sample}.{mode}.bamCoverage.tsv"
    run:
        if {wildcards.mode} == "strict":
            shell("""
            echo '{output}'

            bamCoverage --bam {input.bam} \
            -o  {output} \
            --numberOfProcessors {threads} \
            {params.common_strict} &> {log}
            """)
        else:
            shell("""
            echo '{output}'
            
            bamCoverage --bam {input.bam} \
            -o  {output} \
            --numberOfProcessors {threads} \
            {params.common_liberal} &> {log}
            """)



rule feature_count:
    input:
        bams=expand("mapped_reads/{sample}.bam", sample=SAMPLES), # sort by name -> faster
        gtf=GTF
    output:
        count="feature_count/counts.s{strand}.strict.txt" if config["MODE"] == "strict" else "feature_count/counts.s{strand}.liberal.txt",
        summary="feature_count/counts.s{strand}.strict.txt.summary" if config["MODE"] == "strict" else "feature_count/counts.s{strand}.liberal.txt.summary"
    # conda:
    #   "envs/subread.yaml"  # not compatible with 'run'
    priority:
        100
    params:
        mem_mb="4000",
        common_uniq=" -g gene_id -Q 20 --minOverlap 1 --fracOverlap 0 ",
        #common_mult="-g gene_id -M --minOverlap 1 --fracOverlap 0",
        common_mult=" -g gene_id -M --primary -Q 0 --minOverlap 1 --fracOverlap 0 ",
        pe_strict=" -p -B -C ",
        pe_liberal=" -p",
    threads:
        4
    log:
        "log/feature_count/counts.s{strand}.log"
    benchmark:
        "log/feature_count/benchmark.s{strand}.tsv"

    run:
        if config["PAIR_END"]:
            if config["MODE"] == "strict":
                shell("""
                featureCounts -a {input.gtf} -o {output.count} \
                -T {threads} \
                {params.common_uniq} {params.pe_strict} \
                -s {wildcards.strand} \
                {input.bams} &> {log}
                """)
            elif config["MODE"] == "liberal":
                shell("""
                featureCounts -a {input.gtf} -o {output.count} \
                -T {threads} \
                {params.common_mult} {params.pe_liberal} \
                -s {wildcards.strand} \
                {input.bams} &> {log}
                """)
            else:
                print ("MODE err, not liberal nor strict")
        else:
            shell("""
            # strict
            featureCounts -a {input.gtf} -o {output.count} \
            -T {threads} \
            {params.common_uniq} \
            -s {wildcards.strand} \
            {input.bams} &> {log}
            # liberal
            featureCounts -a {input.gtf} -o {output.count} \
            -T {threads} \
            {params.common_mult} \
            -s {wildcards.strand} \
            {input.bams} &> {log}
            """)
            # -p: count Fragments rather than reads for Paired-end reads (remove this for SE data)
            # -C: exclude chimeric (most times, for cancer maybe not include this)
            # -d 50, -D 1000: including PE reads with fragment length in between, which is better than default 50-600
            # -T: num threads
            # -s: strand info, very important; use $i to perform all three possibilities, pick the correct one after counting
            # -Q: min MAPQ, if MAPQ from star, we need to be careful, because star ignores PE information, we might need to add addional step to rescue PE info. (https://github.com/alexdobin/STAR/issues/615)
            # -M: count multiple-mapping reads, based on NH, not useful for RNA-seq, may create misleading summary, by counting multi-mapping reads several times
            # -B: Only count read pairs that have both ends aligned.
            # --fracOverlap 0.2: 20% of read length
            # -–minOverlap 2: 2bp
            # Notes:
            # liberal overlap settings (--minOverlap 1 --fracOverlap 0) will give you more counted reads
            # samtools sorted bams are smaller, and faster to be counted, compared to unsorted bams/star sorted bams
            # star sorted bams are slow to count => use samtools sorted reads, delete star bam (set as temp)


rule strand_detection:
    input:
        expand("feature_count/counts.s{strand}.strict.txt.summary", strand=STRAND) 
        if config["MODE"] == "strict" 
        else expand("feature_count/counts.s{strand}.liberal.txt.summary", strand=STRAND)
    output:
        "feature_count/counts.strict.txt" if config["MODE"] == "strict" else  "feature_count/counts.liberal.txt",
        "meta/strandness.detected.txt"
    threads:
        1
    params:
        mem_mb=1000
    log:
        "log/strand_detection.log"
    script:
        "script/strandness_detection.py" # todo: improvement needed


def DESeq2_input(config):
    if config["START"] in ["FASTQ", "BAM"]:
        if config["MODE"] == "strict":
            return("feature_count/counts.strict.txt")
        elif config["MODE"] == "liberal":
            return("feature_count/counts.liberal.txt")
        else:
            raise Exception("START config not valid")
    elif config["START"] in ["COUNT"]:
        if config["COUNT_FILE"].endswith("txt"):
            return(config["COUNT_FILE"])
        elif config["COUNT_FILE"].endswith("xlsx"):
            df = pd.read_excel(config["COUNT_FILE"])
            df.to_csv("meta/COUNT.xlsx.txt", sep="\t", index=False, header=True)
            return("meta/COUNT.xlsx.txt")
        else:
            raise NameError('COUNT file not txt nor xlsx format, Exit')
            
    else:
        return("placeholder" )


rule DESeq2:
    # todo: complicated contrasts
    input:
        DESeq2_input(config)
    output:
        "DESeq2/DESeq2.html",
        expand("DESeq2/rnk/{contrast}.rnk", contrast=CONTRASTS_DE)
    # conda:
    #     "envs/r.yaml"
    params:
        mem_mb="8000",
        rmd="'script/DESeq2.Rmd'",
        fdr=MAX_FDR,
        lfc=MIN_LFC,
        anno_tab=ANNO_TAB,
        meta=config["META"],
        cont=config["CONTRAST_DE"],
        o="'../DESeq2/DESeq2.html'"
    container:
        "envs/hand_sandbox.simg"
    priority: 
        100
    threads:
        1
    log:
        "log/DESeq2/DESeq2.log"
    benchmark:
        "log/DESeq2/DESeq2.tsv"
    shell:
        #Rscript -e rmarkdown::render({params.rmd})
        'Rscript -e "rmarkdown::render({params.rmd}, \
        params=list(max_fdr={params.fdr}, min_lfc={params.lfc}, \
        countFile=\'../{input}\', \
        annoFile=\'{params.anno_tab}\', \
        metaFile=\'../{params.meta}\', \
        contrastFile=\'../{params.cont}\'           ), \
        output_file={params.o})" &> {log}'

## GSEA prep (toupper if contain lower, timestamp will change only if contain_lower)
files = os.listdir("meta")
for fname in files:
    fname='meta/'+fname
    if fname.endswith("rnk"):
        inputFile = open(fname, 'r')
        content = inputFile.read()
        contain_lower = False
        for line in content:
            if any(letter.islower() for letter in line):
                contain_lower = True
                break
        if contain_lower:
            with open(fname, 'w') as outputFile:
                outputFile.write(content.upper())        

rule GSEA:
    input: 
        deseq2= "Snakefile" # dummy assume existing
            if config["START"] == "RNK" 
            else "DESeq2/DESeq2.html" , # to keep ordering
        rnk="meta/{contrast}.rnk"  # contrast just a name, alias for rnk
            if config["START"] == "RNK" 
            else "DESeq2/rnk/{contrast}.rnk",
        db="envs/gsea_db/{db}"
    output:
        touch("gsea/{contrast}/{db}.finished")  # todo: premature killed jobs create junk sub folders, confusing to users
        #directory("gsea/{contrast}/{db}")
        #directory(lambda wildcards: "gsea/"+wildcards['contrast']+wildcards['db'][:-12])
    params:
        mem_mb="8000",
        svg=config["GSEA_PLOT_SVG"],
        nplot=config["GSEA_NPLOTS"],
        label_db=lambda wildcards: wildcards["db"][:-12]
    threads:
        1
    log:
        "log/gsea/{contrast}.{db}.log"
    benchmark:
        "log/gsea/{contrast}.{db}.tsv"
    # run:
    #     print('test')
    run:
        shell("echo {params.label_db}")
        shell("mkdir -p gsea && mkdir -p gsea/{wildcards.contrast}")
        shell("""#module purge;             module load  openjdk/11+28; 
            envs/GSEA_4.0.3/gsea-cli.sh GSEAPreranked \
            -gmx {input.db} -rnk {input.rnk} -rpt_label {wildcards.db} \
            -norm meandiv -nperm 1000  -scoring_scheme classic \
            -create_svgs {params.svg} -make_sets true  -rnd_seed timestamp -zip_report false \
            -set_max 15000 -set_min 15 \
            -plot_top_x {params.nplot} -out ./gsea/{wildcards.contrast} &> {log}""")

# todo: if gsea/contrast/err_{db}, touch gsea/contrast/db.finished



# rule read_length_detection_fq: (not needed because can be detected from BAM)
#     input:
#         expand("fastqc/details/{sample}.{r}_fastqc.html", sample=SAMPLES, r=["R1", "R2"])
#             if config['PAIR_END']
#             else expand("fastqc/details/{sample}_fastqc.html", sample=SAMPLES) # todo: test with single end reads
#     output:
#         "meta/read_length.txt"
#     params:
#         mem_mb="1000",
#     threads:
#         1
#     log:
#         "log/read_length_detection/read_length_detection.log"
#     benchmark:
#         "log/read_length_detection/read_length_detection.tsv"
#     run:
#         import re
#         import statistics
#         lengths = []
#         for f in input:
#             p = re.compile("Sequence length</td><td>(\d*)</td></tr>")
#             for i, line in enumerate(open(f, "r")):
#                 for match in re.finditer(p, line):
#                     lengths.append(match.group(1))
#         lengths = list(map(int, lengths))
#         if(len(set(lengths))!=1):
#             sys.exit("Not all fastq files have the same length")
        
#         median_length = int(statistics.median(lengths))
#         print(output[0])
#         with open(output[0], "w") as out:
#             out.write(str(median_length))

rule read_length_detection_bam:
    input:
        expand("bam_qc/stats/{sample}.stats.txt", sample=SAMPLES)
            if config['START'] in ["FASTQ", "BAM"]
            else "BAM file not provided. AND can't be generated from FASTQ, because FASTQ not provided"
    output:
        "meta/read_length.txt"
    params:
        mem_mb="1000",
    threads:
        1
    log:
        "log/read_length_detection_bam/read_length_detection_bam.log"
    benchmark:
        "log/read_length_detection_bam/read_length_detection_bam.tsv"
    run:
        import re
        import statistics
        lengths = []
        for f in input:
            p = re.compile("SN      average length: (\d*)")
            for i, line in enumerate(open(f, "r")):
                for match in re.finditer(p, line):
                    lengths.append(match.group(1))
        lengths = list(map(int, lengths))
        if(len(set(lengths))!=1):
            sys.exit("Not all fastq files have the same length")
        
        median_length = int(statistics.median(lengths))
        print(output[0])
        with open(output[0], "w") as out:
            out.write(str(median_length))

def get_strandness (x):
    with open("meta/strandness.detected.txt", "r") as file:
        txt = file.readline()
    #print("meta/strandness.detected.txt:", txt)
    p = re.compile("counts.s(.)\.[liberal|strict]")
    res = p.search(txt)
    #print("strand code:", res.group(1))
    if res.group(1) is None:
        sys.exit("strandness detection wrong")
    strand = (config['RMATS_STRANDNESS'][int(res.group(1))])
    return (strand)


def read_length(x):
    with open("meta/read_length.txt", "r") as file:
        txt = file.readline()
    #print("read length", int(txt))
    return (int(txt))


rule rMATS:
    #todo: rMATS4.0(sandbox failed because of apt dependency, still got partial results)
    #todo: update to docker: simple to install, faster, recommended on website
    input:
        b1=lambda wildcards: B1S[int(wildcards['ascn'])],
        b2=lambda wildcards: B2S[int(wildcards['ascn'])],
        gtf=GTF, 
        length_file= "Snakefile" if config['READ_LENGTH'] else "meta/read_length.txt" , # Snakefile is placeholder by-passing read_length detection
        strand_file="meta/strandness.detected.txt" if len(config['STRAND']) > 1 else "Snakefile" # placeholder to skip this for dag creation
    output:
        "rMATS.{ascn}/MATS_output/RI.MATS.ReadsOnTargetAndJunctionCounts.txt" # todo: entire list, or folder
    conda:
      "envs/rmats.yaml"
    params:
        mem_mb="8000",
        b1=lambda wildcards: B1[int(wildcards['ascn'])], 
        b2=lambda wildcards: B2[int(wildcards['ascn'])], 
        type="paired" if config['PAIR_END'] else "single",  # todo, rMATS3 has a single end bug
        analysis="P" if config['PAIR_END'] else "U", 
        length=config['READ_LENGTH'] if config['READ_LENGTH'] else read_length, 
        strandness=get_strandness if len(config['STRAND']) > 1 else "Snakefile" # placeholder to skip this for dag creation
    threads:
        1  # todo: multi thread for rMATS4
    log:
        "log/rMATS/rMATS.{ascn}.log"
    benchmark:
        "log/rMATS/rMATS.{ascn}.tsv"
    shell:
        "python2 envs/rMATS.3.2.5/RNASeq-MATS.py -b1 {params.b1} -b2 {params.b2} \
         -gtf {input.gtf} -o rMATS.{wildcards.ascn} \
         -t {params.type} -len {params.length} -c 0.1 -analysis {params.analysis} -libType {params.strandness} &> {log}"
        # conda: snakemake --use-conda -p -j 5 rMATS/bam_test_genecode/ASEvents/fromGTF.A3SS.txt

        # python envs/rMATS.3.2.5/RNASeq-MATS.py -b1 sorted_reads/KO_D8_1_S6.bam,sorted_reads/KO_D8_2_S12.bam,sorted_reads/WT_D8_1_S5.bam,sorted_reads/WT_D8_2_S11.bam -b2  sorted_reads/KO_D0_1_S2.bam,sorted_reads/KO_D0_2_S8.bam,sorted_reads/WT_D0_1_S1.bam,sorted_reads/WT_D0_2_S7.bam -gtf genome/mm10_chr19/genecode.vM21.chr19.gtf -o rMATS -t paired -len 100 -c 0.0001 -analysis paired -libType fr-unstranded

        # v 3.5 have no pvalues, must use 4.0
        # "singularity exec ~/singularity/hand_sandbox/ \
        # /opt/anaconda3/envs/rmat/bin/python /opt/bin/rMATS.4.0.2/rMATS-turbo-Linux-UCS4/rmats.py \
        # --b1 b1.txt --b2 b2.txt \
        # --gtf Homo_sapiens.GRCh37.75.gtf \
        # --od rMATS  -t {params.type}} --readLength 100 --cstat 0.0001 --libType fr-unstranded"


rule create_dag:
    # todo: fix bug
    params:
        mem_mb="1000"  
        # every job has to have this defined 
        # to use snakemake --cluster 'bsub -q short -R "rusage[mem_mb={params.mem_mb}]" -n {threads}'
    threads:
        1
    output:
        "Workflow_DAG.all.svg"
    log:
        "log/create_dag/Workflow_DAG.all.svg.log"
    shell:
        "snakemake --dag targets | dot -Tsvg > {output} 2> {log}"


rule reset:
    log: 
        "log/reset.log"
    run:
        shell("rm -rf fastqc/ bigWig/ bam_qc/ mapped_reads/ sorted_reads/ create_dag/ rMATS.0/ rMATS.1/ rMATS.2/ rMATS.3/ rMATS.4/ rMATS.5/ rMATS.6/ rMATS.7/ \
        lsf.log Log.out meta/strandness.detected.txt Workflow_DAG.all.svg report.html feature_count log nohup.out DESeq2 gsea &> {log}")

rule test:
    input:
        "config.yaml"
    output:
        "test.out"
    params:
        cmd=config['MAX_FDR']
    shell:
        'echo test'
        'echo {params.cmd}'

